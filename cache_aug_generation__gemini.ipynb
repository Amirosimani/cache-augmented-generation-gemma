{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHR5bhCRxdRg5lAaF/6lSA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amirosimani/cache-augmented-generation-gemma/blob/main/cache_aug_generation__gemini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "metadata": {
        "id": "cw9SQb85pS02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "|||\n",
        "|----------|-------------|\n",
        "| Author(s)   | amirimani@ |\n",
        "| Last updated | 8/01/2025 |\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "1mB9Xc9jpVhl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [Don't Do RAG: When Cache-Augmented Generation is All You Need for Knowledge Tasks](https://arxiv.org/abs/2412.15605v1)\n",
        "\n",
        "\n",
        "This paper arguest that by preloading all relevant information into the model's vast context window and using a key-value cache with precomputed inferences, Context-Augmented Generation (CAG) eliminates the need for real-time retrieval in large language models, overcoming the limitations of traditional Retrieval-Augmented Generation.\n",
        "\n",
        "### Concepts\n",
        "**key-value cache**: key-value cache acts as a memory bank for autoregressive generative models, where the model stores key-value pairs derived from self-attention layers for previously processed tokens\n",
        "\n",
        "## Benefits of CAG\n",
        "* Eliminates Real-time Retrieval: Preloads all necessary documents directly into the language model's context.\n",
        "* Boosts Efficiency: Employs a precomputed cache to accelerate response times.\n",
        "* Streamlines Architecture: Removes the need for separate retrieval systems, simplifying the overall process.\n",
        "\n",
        "## Steps:\n",
        "\n",
        "**Preloading External Knowledge:**\n",
        "* preprocess a collection of documents relevant to the application.\n",
        "* Encode these documents into a KV cache, which captures the inference state of the LLM.\n",
        "* Store the KV cache on disk or in memory for reuse during inference.\n",
        "\n",
        "**Inference:**\n",
        "* Load the precomputed KV cache alongside the user’s query.\n",
        "* The LLM processes the query using the preloaded knowledge for contextually accurate responses.\n",
        "  * add a repetition criteria\n",
        "* Combine the user query with the preloaded documents for a unified prompt\n",
        "\n",
        "**Cache Reset:**\n",
        "* Truncate new tokens in the cache without reloading the entire context.\n",
        "\n",
        "\n",
        "## Technical considerations\n",
        "\n",
        "* Quantization: using the HF blog post approach for kv caching [link](https://huggingface.co/blog/kv-cache-quantization#how-to-use-quantized-kv-cache-in-%F0%9F%A4%97-transformers)\n",
        "* Stopping condition: to solve the issue of repetitive output, I added:\n",
        "  - a stopping criteria that terminates generation when encountering a specific pattern, such as a newline or end-of-answer marker.\n",
        "  - Adjusting Decoding Parameters: set repetition_penalty to penalize repeated tokens.\n"
      ],
      "metadata": {
        "id": "ZSEObxT6Y_wE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**based on your choice of model, use a runtime with GPU**"
      ],
      "metadata": {
        "id": "abpfT4lyYYU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U --quiet bitsandbytes accelerate quanto\n",
        "!pip install --quiet transformers\n",
        "!pip install PyPDF2 --quiet"
      ],
      "metadata": {
        "id": "tWh3yE2BQQCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54049569-9baf-438e-de29-0f316714ef57"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    PreTrainedModel,\n",
        "    PreTrainedTokenizer,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    DynamicCache\n",
        ")\n",
        "import torch\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "from typing import Optional, Union\n",
        "from google.colab import userdata\n",
        "\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "3ryIjrVZkNw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# utility functions"
      ],
      "metadata": {
        "id": "XkikatGIkWOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(\n",
        "    model: PreTrainedModel,\n",
        "    input_ids: torch.Tensor,\n",
        "    past_key_values: Optional[DynamicCache] = None,\n",
        "    max_new_tokens: int = 50,\n",
        "    repetition_penalty: float = 1.0,\n",
        "    stop_token: Optional[str] = None,\n",
        "    tokenizer: Optional[PreTrainedTokenizer] = None,\n",
        "    top_p: float = 0.9,\n",
        "    temperature: float = 1.0\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Generate tokens from a model using sampling with past key-value caching.\n",
        "\n",
        "    Args:\n",
        "        model (PreTrainedModel): The language model for generation.\n",
        "        input_ids (torch.Tensor): Input token IDs to begin generation.\n",
        "        past_key_values (Optional[DynamicCache]): Cached key-value pairs for faster inference.\n",
        "        max_new_tokens (int): Maximum number of new tokens to generate.\n",
        "        repetition_penalty (float): Penalty for repeated tokens.\n",
        "        stop_token (Optional[str]): A token to stop generation upon encountering.\n",
        "        tokenizer (Optional[PreTrainedTokenizer]): Tokenizer to decode stop_token.\n",
        "        top_p (float): Probability mass for nucleus sampling.\n",
        "        temperature (float): Sampling temperature.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: The generated tokens excluding the input prompt.\n",
        "    \"\"\"\n",
        "    device = model.device\n",
        "    input_ids = input_ids.to(device)\n",
        "    output_ids = input_ids.clone()\n",
        "    generated_text = \"\" if tokenizer else None\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new_tokens):\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                past_key_values=past_key_values,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # Apply temperature scaling\n",
        "            logits = logits / temperature\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            if repetition_penalty > 1.0:\n",
        "                for token_id in torch.unique(output_ids):\n",
        "                    logits[:, token_id] /= repetition_penalty\n",
        "\n",
        "            # Apply nucleus sampling\n",
        "            probabilities = torch.softmax(logits, dim=-1)\n",
        "            sorted_probs, sorted_indices = torch.sort(probabilities, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "            sorted_indices_to_remove[:, 0] = 0\n",
        "\n",
        "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "            probabilities[:, indices_to_remove] = 0\n",
        "            probabilities = probabilities / probabilities.sum(dim=-1, keepdim=True)\n",
        "\n",
        "            next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "            output_ids = torch.cat([output_ids, next_token], dim=-1)\n",
        "            past_key_values = outputs.past_key_values\n",
        "            input_ids = next_token\n",
        "\n",
        "            # Decode if stop_token is specified\n",
        "            if tokenizer and stop_token:\n",
        "                generated_text += tokenizer.decode(next_token[0], skip_special_tokens=True)\n",
        "                if stop_token in generated_text:\n",
        "                    break\n",
        "\n",
        "            # Stop generation if EOS token is encountered\n",
        "            if model.config.eos_token_id is not None and next_token.item() == model.config.eos_token_id:\n",
        "                break\n",
        "\n",
        "    return output_ids[:, input_ids.shape[-1]:]\n",
        "\n",
        "# def generate(\n",
        "#     model: PreTrainedModel,\n",
        "#     input_ids: torch.Tensor,\n",
        "#     past_key_values: Optional[Union[dict, list]] = None,\n",
        "#     max_new_tokens: int = 50,\n",
        "# ) -> torch.Tensor:\n",
        "#     \"\"\"\n",
        "#     Generate tokens from a model using greedy decoding with past key-value caching.\n",
        "\n",
        "#     Args:\n",
        "#         model (PreTrainedModel): The language model for generation.\n",
        "#         input_ids (torch.Tensor): Input token IDs to begin generation.\n",
        "#         past_key_values (Optional[Union[dict, list]]): Cached key-value pairs for faster inference.\n",
        "#         max_new_tokens (int): Maximum number of new tokens to generate.\n",
        "\n",
        "#     Returns:\n",
        "#         torch.Tensor: The generated tokens excluding the input prompt.\n",
        "#     \"\"\"\n",
        "#     device = model.device\n",
        "#     input_ids = input_ids.to(device)\n",
        "#     output_ids = input_ids.clone()\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for _ in range(max_new_tokens):\n",
        "#             outputs = model(\n",
        "#                 input_ids=input_ids,\n",
        "#                 past_key_values=past_key_values,\n",
        "#                 use_cache=True,\n",
        "#             )\n",
        "#             logits = outputs.logits[:, -1, :]\n",
        "#             next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "#             output_ids = torch.cat([output_ids, next_token], dim=-1)\n",
        "#             past_key_values = outputs.past_key_values\n",
        "#             input_ids = next_token\n",
        "\n",
        "#             # Stop generation if EOS token is encountered\n",
        "#             if model.config.eos_token_id is not None and next_token.item() == model.config.eos_token_id:\n",
        "#                 break\n",
        "\n",
        "#     # Return only newly generated tokens\n",
        "#     return output_ids[:, input_ids.shape[-1]:]\n",
        "\n",
        "def get_kv_cache(model: PreTrainedModel, tokenizer: PreTrainedTokenizer, prompt: str) -> DynamicCache:\n",
        "    \"\"\"\n",
        "    Generate and return key-value cache for a given prompt.\n",
        "\n",
        "    Args:\n",
        "        model (PreTrainedModel): The language model to use.\n",
        "        tokenizer (PreTrainedTokenizer): Tokenizer to encode the prompt.\n",
        "        prompt (str): The prompt text.\n",
        "\n",
        "    Returns:\n",
        "        DynamicCache: A cache containing key-value tensors for faster inference.\n",
        "    \"\"\"\n",
        "    device = model.model.embed_tokens.weight.device\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    cache = DynamicCache()\n",
        "\n",
        "    # Using quantized KV cache\n",
        "    cache_config = {\"backend\": \"quanto\", \"nbits\": 4}\n",
        "    with torch.no_grad():\n",
        "        _ = model(\n",
        "            input_ids=input_ids,\n",
        "            past_key_values=cache,\n",
        "            use_cache=True,\n",
        "            cache_implementation=\"quantized\",\n",
        "            cache_config=cache_config,\n",
        "        )\n",
        "    return cache\n",
        "\n",
        "def clean_up(cache: DynamicCache, origin_len: int):\n",
        "    \"\"\"\n",
        "    Clean up the cache by truncating to the original length.\n",
        "\n",
        "    Args:\n",
        "        cache (DynamicCache): Cache object containing key and value tensors.\n",
        "        origin_len (int): The original sequence length to truncate to.\n",
        "    \"\"\"\n",
        "    for i in range(len(cache.key_cache)):\n",
        "        cache.key_cache[i] = cache.key_cache[i][:, :, :origin_len, :]\n",
        "        cache.value_cache[i] = cache.value_cache[i][:, :, :origin_len, :]"
      ],
      "metadata": {
        "id": "4CRaC3DouLhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model from HuggingFace|"
      ],
      "metadata": {
        "id": "ZSr5mDyRiO_j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/gemma-7b\"\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "# )\n",
        "token = userdata.get('huggingface')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=token, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "    trust_remote_code=True,\n",
        "    token=token,\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "print(f\"Loaded {model_name}.\")"
      ],
      "metadata": {
        "id": "dtl8KwBqleeb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cache external data"
      ],
      "metadata": {
        "id": "yUwjwtCUikWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_from_url(url):\n",
        "  \"\"\"\n",
        "  Reads a PDF from a URL and returns a PdfReader object.\n",
        "\n",
        "  Args:\n",
        "    url: The URL of the PDF to read.\n",
        "\n",
        "  Returns:\n",
        "    A PdfReader object representing the PDF.\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    pdf_file = PdfReader(BytesIO(response.content))\n",
        "    return pdf_file\n",
        "\n",
        "  except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching PDF from URL: {e}\")\n",
        "    return None\n",
        "  except Exception as e:\n",
        "    print(f\"Error reading PDF: {e}\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DDcZaBwXip4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_url = \"https://abc.xyz/assets/1f/d2/b4b2a1c4437395dce09645f71173/2024-q3-earnings-transcript.pdf\" #google 2024 Q3 earnings call\n",
        "pdf_reader = read_pdf_from_url(pdf_url)\n",
        "text_output = pdf_reader.pages[0].extract_text()\n"
      ],
      "metadata": {
        "id": "9ig02d9rivBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_output"
      ],
      "metadata": {
        "id": "r8ba5bIWjCwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"\n",
        "<|system|>\n",
        "You are an assistant who provides concise factual answers.\n",
        "<|user|>\n",
        "Context:\n",
        "{text_output}\n",
        "Question:\n",
        "\"\"\".strip()"
      ],
      "metadata": {
        "id": "vChXetxMlhKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cache = get_kv_cache(model, tokenizer, system_prompt)\n",
        "origin_len = cache.key_cache[0].shape[-2]\n",
        "print(\"KV cache with quantization built.\")"
      ],
      "metadata": {
        "id": "rvu-21OOltDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate answers\n",
        "question = \"what what google cloud's Q3 revenue?\"\n",
        "\n",
        "clean_up(cache, origin_len)\n",
        "input_ids_q = tokenizer(question + \"\\n\", return_tensors=\"pt\").input_ids.to(model.device)\n",
        "gen_ids_q = generate(\n",
        "    model,\n",
        "    input_ids_q,\n",
        "    past_key_values=cache,\n",
        "    max_new_tokens=50,\n",
        "    repetition_penalty=1.0,\n",
        "    stop_token=\"Question\",\n",
        "    tokenizer=tokenizer,\n",
        "    top_p=0.9,\n",
        "    temperature=0.9\n",
        ")\n",
        "answer = tokenizer.decode(gen_ids_q[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "print(\"Q:\", question)\n",
        "print(\"A:\", answer)"
      ],
      "metadata": {
        "id": "eI_oFoT7l1VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chat interfrace"
      ],
      "metadata": {
        "id": "Kwjy35D5YwlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet langchain"
      ],
      "metadata": {
        "id": "sseitLT4YvG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms.base import LLM\n",
        "from typing import Optional, List, Any\n",
        "\n",
        "class CustomLLM(LLM):\n",
        "    def __init__(self, model: Any, tokenizer: Any, cache: Any, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self._model = model  # Use private attributes to bypass pydantic validation\n",
        "        self._tokenizer = tokenizer\n",
        "        self._cache = cache\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_llm\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        # Tokenize the input\n",
        "        input_ids = self._tokenizer(prompt, return_tensors=\"pt\").input_ids.to(self._model.device)\n",
        "\n",
        "        # Generate response\n",
        "        gen_ids = generate(self._model, input_ids, past_key_values=self._cache)\n",
        "\n",
        "        # Decode output\n",
        "        response = self._tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        # Apply stop tokens if provided\n",
        "        if stop:\n",
        "            for stop_token in stop:\n",
        "                response = response.split(stop_token)[0]\n",
        "        return response\n"
      ],
      "metadata": {
        "id": "fxQLogQMxBpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the LLM\n",
        "custom_llm = CustomLLM(model=model, tokenizer=tokenizer, cache=cache)\n",
        "\n",
        "# Create the LLMChain\n",
        "llm_chain = LLMChain(prompt=prompt, llm=custom_llm)\n",
        "\n",
        "# Chat loop\n",
        "context = text_output[:10000]\n",
        "while True:\n",
        "    user_question = input(\"You: \")\n",
        "    if user_question.lower() == \"exit\":\n",
        "        break\n",
        "    response = llm_chain.run(context=context, question=user_question)\n",
        "    print(f\"Assistant: {response}\")"
      ],
      "metadata": {
        "id": "MxpnHA_wxcWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eCZ4yPGpxhPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}